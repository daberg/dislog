\documentclass[UTF8]{article}

\usepackage{amssymb}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage[left=1.5in, right=1.5in, top=0.8in, bottom=1in]{geometry}
\usepackage[nice]{nicefrac}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{xcolor}

\setlength\parindent{0pt}
\setlength{\parskip}{\baselineskip}
\widowpenalties 1 10000

\definecolor{blue}{rgb}{0,0,0.5}
\definecolor{red}{rgb}{0.6,0,0}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    columns=fullflexible,
    frame=L,
    numbers=left,
    emph={exhaustive, babygiant, _map, pollard, pohlighellman},
    otherkeywords={True,False},
    emphstyle=\color{red},
    keywordstyle=\color{blue},
    showstringspaces=false
}

\renewcommand{\figurename}{Python Implementation}

\begin{document}

\title{
    Discrete Logarithm Algorithms \\
    \large Description and Implementation
}
\author{Davide Bergamaschi}
\date{2019}
\maketitle

\begin{abstract}
\noindent In this document we present the cryptographic problem of discrete logarithm and a number of algorithms to solve it, along with our implementation of said algorithms in the Python programming language. In particular, the following algorithms will be examined: exhaustive search, Baby-Step Giant-Step, Pollard's Rho, Pohlig-Hellman.
\end{abstract}

\section{The Discrete Logarithm Problem}

Let $G$ be a cyclic group of order $n$, let $\alpha$ be a generator of G and let $\beta$ belong to G. The discrete logarithm of $\beta$ to the base $\alpha$ ($\log_{\alpha}\beta$) is the integer value $x$ such that $0 \leq x \leq n - 1$ and $\alpha^x=\beta$.

\section{Solving Discrete Logarithms}

Calculating discrete logarithms is in general a computationally hard problem. In the following section we present a selection algorithms that perform this task with different levels of sophistication.

Each algorithm is implemented by a Python function which takes $\alpha$, $\beta$ and $n$ as its first arguments, and returns either $\log_{\alpha}\beta$ or $None$ if the logarithm cannot be found due to input data not respecting preconditions (e.g. if $\alpha$ is not a generator) or in case of failure of a randomized algorithm.

\newpage

\subsection{Exhaustive Search}

This naive method consists in computing $\alpha^0, \alpha^1, \ldots, \alpha^{n - 1}$ until $\beta$ is eventually obtained.

\begin{figure}[H]
    \centering
    \caption{Exhaustive Search}
    \begin{tabular}{c}
        \lstinputlisting{lst1-exhaustive.py}
    \end{tabular}
\end{figure}

The time complexity is trivially given by $O(n)$ multiplications.

\subsection{Baby-Step Giant-Step}

This method relies on the fact that the logarithm $x$ can be written as $x = i m + j$, where $m$ can be conveniently chosen as $\lceil \sqrt{n} \rceil$. By doing so, one obtains $\beta = \alpha^{x} = \alpha^{i m + j} = \alpha^{i m} \alpha^{j}$, which is true if and only if $\beta (\alpha^{-m})^i = \alpha^{j}$.

The algorithm hence proceeds in the following way. For $0 \leq j < m$, entries $(a^j, j)$ are computed and stored in a hash table (hashed on the first component).

Then, for $0 \leq i < \lceil n / m \rceil$, $\beta (\alpha^{-m})^i$ is computed. At each iteration the algorithm checks if the obtained value is present in the hash table. Upon finding a match with a value $j$, the logarithm x is obtained as $x = i m + j$.

\begin{figure}[H]
    \centering
    \caption{Baby-Step Giant-Step}
    \begin{tabular}{c}
        \lstinputlisting{lst2-babygiant.py}
    \end{tabular}
\end{figure}

In the first part, if \verb|m| has not been specified, it is calculated as $\lceil \sqrt{n} \rceil$. The lookup table is then constructed accordingly.

In each iteration of the last part, $\beta (\alpha^{-m})^i$ is computed by multiplying the \verb|candidate| variable, initially set to $\beta$, by the precomputed value $\alpha^{-m}$.

The algorithm performs $O(m)$ group multiplications while constructing the table, and $O(n/m)$ multiplications and lookups in the last part. If $m$ is set to $\sqrt{n}$, the time complexity becomes $O(\sqrt{n})$. The space complexity is instead given by the $m$ group elements stored by the algorithm.

\subsection{Pollard's Rho Algorithm for Logarithms}

This randomized algorithm explores a sequence of group elements $\gamma_i = \alpha^{a_i} \beta^{b_i}$ looking for a cycle using Floyd's algorithm.

Detecting a cycle means discovering two elements $\gamma_c = \alpha^{a}\beta^{b}$ and $\gamma_{2c} = \alpha^{A} \beta^{B}$ in the sequence, such that $\gamma_c = \gamma_{2c}$.

It follows that $\alpha^{a} \beta^{b} = \alpha^{A} \beta^{B}$, and so $\beta^{b - B} = \alpha^{a - A}$. By taking the logarithm to the base of $\alpha$ of both sides of the equation one obtains the following equation:
$$(b - B) \log_\alpha{\beta} \equiv (a - A) \pmod{n},$$
which can be solved to find $x=\log_\alpha{\beta}$, but only if $b \not\equiv B \pmod{n}$.

Let us discuss how the sequence of group elements is generated. To increase the likelihood of obtaining a cycle in a small number of samples, G is partitioned into three disjoint subsets $S_0$, $S_1$ and $S_2$ of approximately equal size and chosen in a sufficiently ``random'' manner.

The map for group elements $f: G \rightarrow G$ is then defined as:
$$
f(\gamma) =
\begin{cases}
    \beta \gamma  & \enspace \text{if } \gamma \in S_0 \\
    \gamma^2      & \enspace \text{if } \gamma \in S_1 \\
    \alpha \gamma & \enspace \text{if } \gamma \in S_2
\end{cases},
$$

while the map for $a_i$ coefficients $g: G \times \mathbb{N} \rightarrow \mathbb{N}$ and the map for $b_i$ coefficients $h: G \times \mathbb{N} \rightarrow \mathbb{N}$ are consequently defined as:
$$
g(\gamma, a) =
\begin{cases}
    a                & \enspace \text{if } \gamma \in S_0 \\
    (2a)    \bmod{n} & \enspace \text{if } \gamma \in S_1 \\
    (a + 1) \bmod{n} & \enspace \text{if } \gamma \in S_2
\end{cases},
\qquad
h(\gamma, b) =
\begin{cases}
    (b + 1) \bmod{n} & \enspace \text{if } \gamma \in S_0 \\
    (2b)    \bmod{n} & \enspace \text{if } \gamma \in S_1 \\
    b                & \enspace \text{if } \gamma \in S_2
\end{cases}.
$$

The algorithm begins by iterating over the sequence induced by the above maps, starting from two random coefficients $a_0$ and $b_0$ (and with $x_0 = \alpha^{a_0} \beta^{b_0}$); in particular, at each step $i$ it computes:
\begin{align*}
    \gamma_{i}  & = f(\gamma_{i-1})  & a_{i}  & = g(\gamma_{i-1}, a_{i-1})   & b_{i}  & = h(\gamma_{i-1}, a_{i-1})   \\
    \gamma_{2i} & = f(\gamma_{2i-2}) & a_{2i} & = g(\gamma_{2i-2}, a_{2i-2}) & b_{2i} & = h(\gamma_{2i-2}, b_{2i-2}),
\end{align*}
and compares the obtained $\gamma_{i}$ and $\gamma_{2i}$ values.

When the algorithm finds $\gamma_{i} = \gamma_{2i}$, that is when a cycle is detected, if ($b_{i} - b_{2i}) \bmod{n} \not= 0$ the algorithm returns the logarithm:
$$x = (b_{i} - b_{2i})^{-1} (a_{2i} - a_{i}) \bmod{n},$$
otherwise it terminates with failure.

\begin{figure}[H]
    \centering
    \caption{Pollard's Rho Algorithm for Logarithms}
    \begin{tabular}{c}
        \lstinputlisting{lst3-pollard.py}
    \end{tabular}
\end{figure}

The \verb|s_map| argument is a function which takes a group element as input and returns the index of the partition to which the element belongs.

For what concerns asymptotic complexity, if it can be assumed that the map $f$ behaves like a random function, the expected running time will be of $O(\sqrt{n})$ group operations. On the other hand, the required storage is negligible.

\subsection{Pohlig-Hellman Algorithm}

This method leverages the prime factorization of the group order: $n = p_1^{e_1} \ldots p_r^{e_r}$.

For $1 \leq i \leq r$, the algorithm calculates $x_i = x \bmod{p_i^{e_i}}$, obtaining a set of congruences that can ultimately be solved with the Chinese Remainder Theorem to find $x$.

In particular each $x_i$ can be seen as a truncated $p_i$-ary representation of $x$:
$$x_i = x \bmod{p_i^{e_i}} = l_0 + l_1 p_i + l_2 p_i^2 + \ldots + l_{e_i - 1} p_i^{e_i - 1},$$
which allows for a further reduction of the magnitude of the calculations.

Let us examine how the digits are calculated.

From the definition of modulus, there exists some integer $h$ for which $x=x_i+hp_i^{e_i}$. Since \mbox{$\beta=\alpha^x$}, one can derive:
\begin{align*}
    \beta^{n/p_i} & = (\alpha^x)^{n/p_i} \\
                  & = (\alpha^{n/p_i})^x \\
                  & = (\alpha^{n/p_i})^{x_i+hp_i^{e_i}} \\
                  & = (\alpha^{n/p_i})^{l_0 + l_1 p_i + l_2 p_i^2 + \ldots + l_{e_i - 1} p_i^{e_i - 1}+hp_i^{e_i}} \\
                  & = (\alpha^{n/p_i})^{l_0 + K p_i} \quad \text{\small{(for some integer $K$)}}  \\
                  & = (\alpha^{n/p_i})^{l_0} (\alpha^{n/p_i})^{K p_i} \\
                  & = (\alpha^{n/p_i})^{l_0} \quad \text{\small{(since $\alpha^{n/p_i}$ is of order $p_i$)}},
\end{align*}
from which $l_0$ can be found using any other discrete logarithm technique.

Then, starting from $\beta\alpha^{-l_0} = \alpha^{l_1 p_i + l_2 p_i^2 + \ldots + l_{e_i - 1} p_i^{e_i - 1}+hp_i^{e_i}}$ and by raising both sides to the power of $\nicefrac{n}{p_i^2}$, one can obtain $(\alpha^{n/p_i})^{l_1}=(\beta\alpha^{-l_0})^{n/p_i^2}$ and ultimately $l_1$ in a similar way.

The remaining digits can be computed analogously.

The calculations performed for each $p_i^{e_i}$ in the first part of the algorithm are in conclusion the following:
\begin{align*}
    l_0         &              = \log_{\alpha^{n / p_i}}(\beta)^{n / p_i} \\
    l_1         &              = \log_{\alpha^{n / p_i}}(\beta\alpha^{-l_0})^{n / p_i^2} \\
    l_2         &              = \log_{\alpha^{n / p_i}}(\beta\alpha^{-l_0 - l_1 \, p_i})^{n / p_i^3} \\
                & \vdotswithin{=} \\
    l_{e_i - 1} &              = \log_{\alpha^{n / p_i}}(\beta\alpha^{-l_0 - l_1 \, p_i - \, \ldots \, - l_{e_i - 2} \ p_i^{e_i - 2}})^{n / p_i^{e_i}}.
\end{align*}

Determining all the $x_i$ values yields a set of congruences of the following type:
\begin{align*}
    x & \equiv x_1 \pmod{p_1^{e_1}} \\
      & \vdotswithin{\equiv}        \\
    x & \equiv x_r \pmod{p_r^{e_r}}.
\end{align*}

Since the moduli are pairwise coprime by construction, the Chinese Remainder Theorem guarantees the existence of a solution $x$, which can be found with any suitable computational method (e.g. Gauss's algorithm).

\begin{figure}[H]
    \centering
    \caption{Pohlig-Hellman Algorithm}
    \begin{tabular}{c}
        \lstinputlisting{lst4-pohlighellman.py}
    \end{tabular}
\end{figure}

The prime factorization of $n$ is passed as fourth argument in the form of a Python dictionary, with keys being the prime factors and values are respectively their exponents.

Notice that during the computation of an $x_i$ value, at each iteration $j$ in the internal loop the implementation keeps track of the necessary powers of $p_i$ ($p_i^{j-1}$, $p_i^j$ and $p_i^{j+1}$), so that only one multiplication is enough to calculate them at each step.

The running time is given by $O(\sum_{i=1}^{r}e_i(\lg n + \sqrt{p_i}))$ group multiplications.

\end{document}
